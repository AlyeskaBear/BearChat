{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1508aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas scikit-learn nltk gtts requests Keras tensorflow pydub -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70709c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Define the package names\n",
    "package_names = ['pandas', 'scikit-learn', 'nltk', 'gtts', 'requests', 'keras', 'tensorflow']\n",
    "\n",
    "# Get the paths of the packages\n",
    "package_paths = []\n",
    "for package_name in package_names:\n",
    "    path = !pip show {package_name} | grep Location | cut -d \" \" -f 2\n",
    "    package_paths.append(path[0])\n",
    "\n",
    "# Add the package paths to sys.path\n",
    "for package_path in package_paths:\n",
    "    if package_path not in sys.path:\n",
    "        sys.path.append(package_path)\n",
    "\n",
    "# Print the updated sys.path\n",
    "# print(\"Updated sys.path:\")\n",
    "# for p in sys.path:\n",
    "#    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d5206ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk as nltk\n",
    "from nltk import corpus, stem\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from keras.layers import Input, LSTM, Dense, Embedding\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "from gtts import gTTS\n",
    "from pydub import AudioSegment\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad2f3c83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9783"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the qna_chitchat_friendly data\n",
    "url = \"https://filedn.com/lJpzjOtA91quQEpwdrgCvcy/Business%20Data%20Mining%20and%20Knowledge%20Discovery/RNoteBook/qna_chitchat_friendly.csv\"\n",
    "data = pd.read_csv(url)\n",
    "# size of the dataset\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20823001",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\gentl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "# download the punkt_tab resource,\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Select the \"Question\" column in the text document\n",
    "corpus_Q = data[\"Question\"]\n",
    "\n",
    "# Preprocess the text\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = stem.SnowballStemmer('english')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lower case\n",
    "    text = text.lower()\n",
    "    # Remove numbers\n",
    "    text = ''.join([i for i in text if not i.isdigit()])\n",
    "    # Remove punctuations\n",
    "    text = ''.join([i for i in text if i.isalpha() or i.isspace()])\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stop words\n",
    "    tokens = [i for i in tokens if not i in stop_words]\n",
    "    # Stem the tokens\n",
    "    tokens = [stemmer.stem(i) for i in tokens]\n",
    "    # Join the tokens into a string\n",
    "    text = \" \".join(tokens)\n",
    "    return text\n",
    "\n",
    "corpus_Q = corpus_Q.apply(preprocess_text);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6558d752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the questions\n",
    "tokenizer_q = Tokenizer()\n",
    "tokenizer_q.fit_on_texts(corpus_Q)\n",
    "vocab_size = len(tokenizer_q.word_index) + 1\n",
    "\n",
    "# Convert question to sequences of tokens\n",
    "input_sequences = tokenizer_q.texts_to_sequences(corpus_Q)\n",
    "\n",
    "# Pad the sequences to have equal length\n",
    "max_sequence_length = max([len(seq) for seq in input_sequences])\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "199ec271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the answer sequences\n",
    "# Add start and end tokens to the answers\n",
    "y = data[\"Answer\"].apply(lambda x: 'start ' + x + ' end')\n",
    "\n",
    "# Tokenize the answers\n",
    "tokenizer_a = Tokenizer(filters='')\n",
    "tokenizer_a.fit_on_texts(y)\n",
    "num_classes = len(tokenizer_a.word_index) + 1\n",
    "\n",
    "# Convert answers to sequences of tokens\n",
    "y_encoded = tokenizer_a.texts_to_sequences(y)\n",
    "\n",
    "# Pad the sequences to have equal length\n",
    "max_ans_length = max([len(seq) for seq in y_encoded])\n",
    "y_padded = pad_sequences(y_encoded, maxlen=max_ans_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb6ac367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(input_sequences, y_padded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c3243f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a seq2seq model with encoder and decoder LSTMs\n",
    "embedding_dim = 128\n",
    "lstm_units = 256\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_sequence_length,))\n",
    "encoder_embedding = Embedding(vocab_size, embedding_dim)(encoder_inputs)\n",
    "encoder_lstm = LSTM(lstm_units, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Decoder (Training)\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "decoder_embedding1 = Embedding(num_classes, embedding_dim)(decoder_inputs)\n",
    "decoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding1, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_classes, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Decoder (Inference)\n",
    "decoder_state_input_h = Input(shape=(lstm_units,))\n",
    "decoder_state_input_c = Input(shape=(lstm_units,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_inputs2 = Input(shape=(None,))\n",
    "decoder_embedding2 = Embedding(num_classes, embedding_dim)(decoder_inputs2)\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(decoder_embedding2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "\n",
    "decoder_model = Model([decoder_inputs2] + decoder_states_inputs, [decoder_outputs2] + decoder_states2)\n",
    "\n",
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Prepare the decoder input data with the same length as the output data, but shifted one timestep forward\n",
    "decoder_input_data = np.zeros_like(y_padded)\n",
    "decoder_input_data[:, 1:] = y_padded[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e06c62e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 133ms/step - accuracy: 0.6982 - loss: 2.4025 - val_accuracy: 0.7599 - val_loss: 1.3491\n",
      "Epoch 2/50\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 128ms/step - accuracy: 0.7570 - loss: 1.2976 - val_accuracy: 0.7618 - val_loss: 1.2042\n",
      "Epoch 3/50\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 133ms/step - accuracy: 0.7594 - loss: 1.1415 - val_accuracy: 0.7676 - val_loss: 1.0636\n",
      "Epoch 4/50\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 127ms/step - accuracy: 0.7660 - loss: 1.0124 - val_accuracy: 0.7744 - val_loss: 0.9676\n",
      "Epoch 5/50\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 125ms/step - accuracy: 0.7753 - loss: 0.9147 - val_accuracy: 0.7795 - val_loss: 0.9163\n",
      "Epoch 6/50\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 127ms/step - accuracy: 0.7814 - loss: 0.8535 - val_accuracy: 0.7834 - val_loss: 0.8839\n",
      "Epoch 7/50\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 127ms/step - accuracy: 0.7985 - loss: 0.7809 - val_accuracy: 0.7947 - val_loss: 0.8231\n",
      "Epoch 8/50\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 125ms/step - accuracy: 0.8145 - loss: 0.7074 - val_accuracy: 0.8130 - val_loss: 0.7520\n",
      "Epoch 9/50\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 125ms/step - accuracy: 0.8320 - loss: 0.6348 - val_accuracy: 0.8251 - val_loss: 0.7078\n",
      "Epoch 10/50\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 133ms/step - accuracy: 0.8456 - loss: 0.5780 - val_accuracy: 0.8428 - val_loss: 0.6431\n",
      "Epoch 11/50\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 126ms/step - accuracy: 0.8657 - loss: 0.5116 - val_accuracy: 0.8548 - val_loss: 0.6020\n",
      "Epoch 12/50\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 126ms/step - accuracy: 0.8891 - loss: 0.4213 - val_accuracy: 0.8680 - val_loss: 0.5563\n",
      "Epoch 13/50\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 125ms/step - accuracy: 0.8978 - loss: 0.3871 - val_accuracy: 0.8733 - val_loss: 0.5463\n",
      "Epoch 14/50\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 126ms/step - accuracy: 0.9107 - loss: 0.3421 - val_accuracy: 0.8868 - val_loss: 0.4975\n",
      "Epoch 15/50\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 127ms/step - accuracy: 0.9226 - loss: 0.3004 - val_accuracy: 0.8905 - val_loss: 0.4817\n",
      "Epoch 16/50\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 129ms/step - accuracy: 0.9269 - loss: 0.2858 - val_accuracy: 0.8893 - val_loss: 0.4981\n",
      "Epoch 17/50\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 126ms/step - accuracy: 0.9230 - loss: 0.2959 - val_accuracy: 0.8955 - val_loss: 0.4611\n",
      "Epoch 18/50\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 126ms/step - accuracy: 0.9425 - loss: 0.2278 - val_accuracy: 0.8995 - val_loss: 0.4522\n",
      "Epoch 19/50\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 128ms/step - accuracy: 0.9451 - loss: 0.2124 - val_accuracy: 0.9034 - val_loss: 0.4331\n",
      "Epoch 20/50\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 128ms/step - accuracy: 0.9496 - loss: 0.1910 - val_accuracy: 0.9023 - val_loss: 0.4318\n",
      "Epoch 21/50\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 127ms/step - accuracy: 0.9570 - loss: 0.1631 - val_accuracy: 0.9095 - val_loss: 0.4273\n",
      "Epoch 22/50\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 125ms/step - accuracy: 0.9580 - loss: 0.1620 - val_accuracy: 0.9136 - val_loss: 0.4129\n",
      "Epoch 23/50\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 126ms/step - accuracy: 0.9639 - loss: 0.1378 - val_accuracy: 0.9136 - val_loss: 0.4155\n",
      "Epoch 24/50\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 126ms/step - accuracy: 0.9679 - loss: 0.1233 - val_accuracy: 0.9092 - val_loss: 0.4113\n",
      "Epoch 25/50\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 127ms/step - accuracy: 0.9709 - loss: 0.1148 - val_accuracy: 0.9075 - val_loss: 0.4238\n",
      "Epoch 26/50\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 125ms/step - accuracy: 0.9550 - loss: 0.1610 - val_accuracy: 0.9113 - val_loss: 0.4171\n",
      "Epoch 27/50\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 126ms/step - accuracy: 0.9700 - loss: 0.1117 - val_accuracy: 0.9153 - val_loss: 0.4133\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "history = model.fit([X_train, decoder_input_data], np.expand_dims(y_train, -1), epochs=50, batch_size=64, validation_split=0.1, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c86c41",
   "metadata": {},
   "source": [
    "Observe the improvement of the model's loss and accuracy on the training data fro iteration to iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4b8e2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.21\n"
     ]
    }
   ],
   "source": [
    "# Prepare decoder_input_data_test\n",
    "decoder_input_data_test = np.zeros((len(y_test), max_sequence_length), dtype='int32')\n",
    "for i, target_sequence in enumerate(y_test):\n",
    "    for t, token in enumerate(target_sequence):\n",
    "        if t < max_sequence_length - 1:\n",
    "            decoder_input_data_test[i, t] = token\n",
    "\n",
    "# Reshape y_test\n",
    "y_test = pad_sequences(y_test, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "loss, accuracy = model.evaluate([X_test, decoder_input_data_test], np.expand_dims(y_test, -1), verbose = 0)\n",
    "print(f\"Test set accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a6aad4",
   "metadata": {},
   "source": [
    "The poor performance of the model on the test data illustrates the problem of model overfitting, which is a common issue with network-based models as we discussed, especially when the size of the training dataset is small, as in the case of this study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b315a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_answer_seq2seq(question, play_audio=True):\n",
    "    def decode_sequence(input_seq):\n",
    "        states_value = encoder_model.predict(input_seq, verbose = 0)\n",
    "\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = tokenizer_a.word_index['start']\n",
    "\n",
    "        stop_condition = False\n",
    "        decoded_sentence = []\n",
    "        while not stop_condition:\n",
    "            output_tokens, h, c = decoder_model.predict([target_seq] + states_value, verbose = 0)\n",
    "\n",
    "            sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "            sampled_word = tokenizer_a.index_word.get(sampled_token_index, '')\n",
    "\n",
    "            if sampled_word == 'end' or len(decoded_sentence) > max_sequence_length:\n",
    "                stop_condition = True\n",
    "            else:\n",
    "                decoded_sentence.append(sampled_word)\n",
    "\n",
    "            target_seq = np.zeros((1, 1))\n",
    "            target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "            states_value = [h, c]\n",
    "\n",
    "        return \" \".join(decoded_sentence[1:])\n",
    "\n",
    "    input_question = preprocess_text(question)\n",
    "    input_seq = tokenizer_q.texts_to_sequences([input_question])\n",
    "    input_seq = pad_sequences(input_seq, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "    answer = decode_sequence(input_seq)\n",
    "\n",
    "    # Synthesize speech\n",
    "    tts = gTTS(answer)\n",
    "    tts.save('answer.wav')\n",
    "\n",
    "    # Play audio using ffplay\n",
    "    if play_audio:\n",
    "        os.system('ffplay -nodisp -autoexit answer.wav')\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bcb061cb-9f0b-4dc6-8517-91f4a5718e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oh, i don't have a name.\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "question = \"What is your name?\"\n",
    "answer = predict_answer_seq2seq(question)\n",
    "print(f\"{answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fe220762-b9d7-4629-a12c-cfd564a0e77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i'm digital, to chat and to here.\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "question = \"where are you from?\"\n",
    "answer = predict_answer_seq2seq(question)\n",
    "print(f\"{answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9821d936-cf04-429c-b7f5-05e1b894127a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i don't know you, but i enjoy\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "question = \"how old are you?\"\n",
    "answer = predict_answer_seq2seq(question)\n",
    "print(f\"{answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0e7db09d-b083-4ff5-96b4-4cce7260871c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you're you're pretty \n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "question = \"I really like you!\"\n",
    "answer = predict_answer_seq2seq(question)\n",
    "print(f\"{answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "52efce92-32b4-4309-8e80-ef3bfb97d54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i don't you should follow your heart.\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "question = \"where should we go for dinner?\"\n",
    "answer = predict_answer_seq2seq(question)\n",
    "print(f\"{answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6e23dae1-dc76-47e8-9455-8db4cbabefac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i having a hard time imagining how\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "question = \"who will win 2028 presidental election?\"\n",
    "answer = predict_answer_seq2seq(question)\n",
    "print(f\"{answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9abf9f89-0be3-4c84-91df-549f33df5436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i only thing i'm committed to is\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "question = \"why Maine the best state in the US?\"\n",
    "answer = predict_answer_seq2seq(question)\n",
    "print(f\"{answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7ab38ca7-edc9-48a0-b54a-8ccfcc08424e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i haven't of any other bots, but\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "question = \"will AI change the world?\"\n",
    "answer = predict_answer_seq2seq(question)\n",
    "print(f\"{answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548ec8d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
